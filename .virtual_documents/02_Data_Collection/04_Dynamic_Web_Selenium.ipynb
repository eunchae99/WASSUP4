


pip install selenium


from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import time

options = Options()
options


#특정함수안에서  드라이버 생성시 함수종료될 때 브라우저 같이 종료되는 문제 대응
options.add_experimental_option("detach", True)





url = 'https://naver.com'
driver = webdriver.Chrome(options=options)


driver.get(url)
time.sleep(2)





driver.back()


driver.forward()


driver.refresh()





driver.find_element?


driver.find_element(By.ID, 'query')


# 키 입력
driver.find_element(By.ID, 'query').send_keys('뉴진스')


driver.find_element(By.CLASS_NAME, 'search_input').send_keys('블랙핑크')


driver.find_element(By.NAME, 'query').send_keys('르세라핌')


driver.find_element(By.CSS_SELECTOR, '#query').send_keys('에스파')


driver.find_element(By.CSS_SELECTOR, '.search_input').send_keys('세븐틴')


driver.find_element(By.CSS_SELECTOR, "[title='검색어를 입력해 주세요.']").send_keys('트와이스')


driver.find_element(By.XPATH, '//*[@id="query"]').send_keys('BTS')





driver.find_element(By.LINK_TEXT, '쇼핑')


driver.find_element(By.LINK_TEXT, '쇼핑').click()


# 일부 매칭
driver.find_element(By.PARTIAL_LINK_TEXT, '증').click()


# 태그로 찾기 
# 태그는 요소가 너무 많으므로 정확하게 대상을 찾을 때에는 권장 안함
driver.find_element(By.TAG_NAME, 'div')


# 여러개의 요소를 찾을 때
driver.find_elements(By.CSS_SELECTOR, '.link_service')


driver.find_elements(By.CSS_SELECTOR, '.link_service')[0].get_attribute('href')


links = driver.find_elements(By.CSS_SELECTOR, '.link_service')
for link in links:
    print(link.get_attribute('href'))





#  테스트용 html
url = 'file:///C:/workspace/WASSUP4/02_Data_Collection/sample/signin.html'
driver = webdriver.Chrome(options=options)
driver.get(url)
time.sleep(2)


username = driver.find_element(By.NAME, "username")
username.send_keys('korea')


password = driver.find_element(By.NAME, "password")
password.send_keys('1234')


login = driver.find_element(By.XPATH, '//*[@id="loginForm"]/input[3]')
login.click()


driver.back()


login = driver.find_element(By.CSS_SELECTOR, '[value=Login]')
login.click()


driver.back()


username.clear()


username = driver.find_element(By.NAME, "username")
username.send_keys('korea')
password = driver.find_element(By.NAME, "password")
password.send_keys('1234')


username.submit()


driver.back()


driver.find_element(By.TAG_NAME, 'p').text


# html 소스 추출하기
driver.page_source


driver.close()





from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import time

options = Options()
options.add_argument("--headless=new") 
# options.add_argument('--window-size= x, y') #실행되는 브라우저 크기를 지정할 수 있습니다.
# options.add_argument('--start-maximized') #브라우저가 최대화된 상태로 실행됩니다.
# options.add_argument('--start-fullscreen') #브라우저가 풀스크린 모드(F11)로 실행됩니다.
# options.add_argument('--blink-settings=imagesEnabled=false') #브라우저에서 이미지 로딩을 하지 않습니다.
# options.add_argument('--mute-audio') #브라우저에 음소거 옵션을 적용합니다.
# options.add_argument('incognito') #시크릿 모드의 브라우저가 실행됩니다.
options.add_experimental_option("detach", True) #특정함수안에서  드라이버 생성시 함수종료될 때 브라우저 같이 종료되는 문제 대응


url = 'http://naver.com'
driver = webdriver.Chrome(options=options)
driver.get(url)


print(driver.title)   #백그라운드에서 실행중


driver.quit() # 탭 닫기





from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import time

options = Options()
# options.add_argument("--start-maximized")
# options.add_argument("--headless=new") 
options.add_experimental_option("detach", True) #

url = 'http://naver.com'
driver = webdriver.Chrome(options=options)
driver.get(url)
time.sleep(2)


# 창의 너비/높이 구하기
size = driver.get_window_size()
width = size.get("width")
height = size.get("height")

print(width, height)


# 창 크기 조절
driver.set_window_size(800, 600)


# 스크린 상에서의 창 좌표
position = driver.get_window_position()
x = position.get('x')
y = position.get('y')
print(x,y)


driver.set_window_position(0,0)


# 창 크기 최대화
driver.maximize_window()


# 창 크기 최소화
driver.minimize_window()


# 전체 화면
driver.fullscreen_window()


# 스크린 샷
driver.save_screenshot('./image.png')


driver.close()





from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import time, random, pandas as pd

options = Options()
options.add_argument('--window-size=974,1047')
options.add_argument('--window-position=-0,40')
options.add_experimental_option("detach", True)


# 웹드라이버 로드
driver = webdriver.Chrome( options = options)


# 조건 설정
where = 'blog'
query = '인공지능'
dateform = '20240101to20240927'
url = f'https://search.naver.com/search.naver?ssc=tab.{where}.all&query={query}&sm=tab_opt&nso=so%3Ar%2Cp%3Afrom{dateform}'
# url = f'https://search.naver.com/search.naver?where={where}&query={query}&sm=tab_op&nso=so:r,p:from{dateform}'
fname = f'{where}_{query}_{dateform}'


# url 접속
driver.get(url)
time.sleep(random.randint(2,3))





#네이버 뷰는 최대 1050까지만 노출
# 스크롤 10번
for i in range(10):
    driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')
    time.sleep(random.randint(2, 3))





# get_view()
views = driver.find_elements(By.CSS_SELECTOR, '.lst_view .view_wrap')
result = []

for view in views:
    con_dict = {}
    con_dict['title'] = view.find_element(By.CSS_SELECTOR, '.title_link').text
    con_dict['text'] = view.find_element(By.CSS_SELECTOR, '.dsc_link').text
    con_dict['date'] = view.find_element(By.CSS_SELECTOR, '.sub').text
    result.append(con_dict)
    print(con_dict)
    
print('완료')


#저장된 게시물 리스트를 데이터프레임으로 변환후 csv로 저장
df = pd.DataFrame(result)
df


#데이터 프레임 저장
# csv는 sep 쉼표가 반드시 있어야 한글이 안깨짐
df.to_csv(f'output/naver_{fname}.csv', sep=',', encoding='utf-8-sig')





from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import math, time

options = Options()
options.add_argument('--window-size=974,1047')
options.add_argument('--window-position=-7,0')
options.add_experimental_option("detach", True)


search = input('검색어를 입력하세요.')


URL = 'https://korean.visitkorea.or.kr/search/search_list.do?keyword='+search

driver = webdriver.Chrome(options=options)
driver.get(URL)
time.sleep(3)


# 여행기사 더보기 클릭
# driver.find_element(By.CSS_SELECTOR, ".more_view > a").click()
driver.find_element(By.CSS_SELECTOR, "#s_recommend > .more_view > a").click()


result = driver.find_elements(By.CSS_SELECTOR, '.tit a')
result[0].text


result = driver.find_elements(By.CSS_SELECTOR, '.tit a')
for i in range(len(result)):
    print(result[i].text)


a = driver.find_element(By.XPATH, '//*[@id="search_result"]/ul/li[1]/div[1]/div[1]/a')
a.text


# tit_xpath = '//*[@id="search_result"]/ul/li/div/div/a' # 해당 요소가 하나씩일 때
tit_xpath = '//*[@id="search_result"]/ul/li[*]/div[1]/div[1]/a'
result = driver.find_elements(By.XPATH, tit_xpath)
len(result), result[0].text, result[1].text


for i, title in enumerate(result, 1):
#     print(i, title.text)
    print(i, title.text)


# 2번 페이지 버튼 클릭해보자
driver.find_element(By.XPATH, '/html/body/div[3]/div/div[1]/div[14]/a[2]').click()


cnt = int(input('스크래핑 건수는 몇건입니까?: '))
cnt


page_cnt = math.ceil(cnt/10)
print(page_cnt)


tit_xpath = '//*[@id="search_result"]/ul/li[*]/div[1]/div[1]/a'
no = 0

for x in range(1, page_cnt+1):
    print(f'========= {x} 페이지 작업 =========')
    mylist = driver.find_elements(By.XPATH, tit_xpath)
    
    for item in mylist:
        no += 1
        if no > cnt:
            break
        print(no, item.text)
    
    if no <= cnt:
        a = f'/html/body/div[3]/div/div[1]/div[14]/a[{x+1}]'
        driver.find_element(By.XPATH, a).click()
#         next_button = driver.find_element(By.CSS_SELECTOR, f"a[id='{x+1}']")
#         driver.execute_script("arguments[0].click();", next_button)
        time.sleep(2)
    
print('========= 작업 완료 =========')
# driver.close()


# xpath대신 execute_script함수 사용
tit_xpath = '//*[@id="search_result"]/ul/li[*]/div[1]/div[1]/a'
no = 0

for x in range(1, page_cnt+1):
    print(f'========= {x} 페이지 작업 =========')
    mylist = driver.find_elements(By.XPATH, tit_xpath)
    
    for item in mylist:
        no += 1
        if no > cnt:
            break
        print(no, item.text)
    
    if no <= cnt:
        next_button = driver.find_element(By.CSS_SELECTOR, f"a[id='{x+1}']")
        driver.execute_script("arguments[0].click();", next_button)
        time.sleep(2)
        
print('========= 작업 완료 =========')
driver.close()


from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import math, time

options = Options()
options.add_argument('--window-size=974,1047')
options.add_argument('--window-position=-7,0')
options.add_experimental_option("detach", True)


search = input('검색어를 입력하세요.')


URL = 'https://korean.visitkorea.or.kr/search/search_list.do?keyword='+search

driver = webdriver.Chrome(options=options)
driver.get(URL)
time.sleep(3)


# 여행기사 더보기 클릭
# driver.find_element(By.CSS_SELECTOR, ".more_view > a").click()
driver.find_element(By.CSS_SELECTOR, "#s_recommend > .more_view > a").click()


scrap_cnt = int(input('스크래핑 건수는 몇건입니까?: '))
scrap_cnt


page_cnt = math.ceil(scrap_cnt/10)
print(page_cnt)


driver.find_element(By.CSS_SELECTOR, '.btn_next').click()


# 왜 50번 이상 콘텐츠는 수집이 안될까? full xpath를 분석해보자
# 첫번째 이유 : xpath 좌표는 버튼 개수 기준이므로 page번호와 다름
# //*[@id="7"]
# /html/body/div[3]/div/div[1]/div[14]/a[4]
tit_xpath = '//*[@id="search_result"]/ul/li[*]/div[1]/div[1]/a'
no = 0

for x in range(1, page_cnt+1):
    print(f'========= {x} 페이지 작업 =========')
    mylist = driver.find_elements(By.XPATH, tit_xpath)
    
    for item in mylist:
        no += 1
        if no > scrap_cnt:
            break
        print(no, item.text)
    
    if no <= scrap_cnt:
        print(no)
        if no % 50 == 0:
            driver.find_element(By.CSS_SELECTOR, '.btn_next').click()
            
        a = f'/html/body/div[3]/div/div[1]/div[14]/a[{x+1}]'
        driver.find_element(By.XPATH, a).click()
        #next_button = driver.find_element(By.CSS_SELECTOR, f"a[id='{x+1}']")
        #driver.execute_script("arguments[0].click();", next_button)
        time.sleep(2)

print('========= 작업 완료 =========')
# driver.close()


# 두번째 이유 : no가 50번일 때 다음 버튼을 클릭하고도, 다른 버튼 클릭을 시도함
tit_xpath = '//*[@id="search_result"]/ul/li[*]/div[1]/div[1]/a'
no = 0

for x in range(1, page_cnt+1):
    print(f'========= {x} 페이지 작업 =========')
    mylist = driver.find_elements(By.XPATH, tit_xpath)
    
    for item in mylist:
        no += 1
        if no > scrap_cnt:
            break
        print(no, item.text)
    
    if no <= scrap_cnt:
        print(no)
        if no % 50 == 0:
            driver.find_element(By.CSS_SELECTOR, '.btn_next').click()
            
        # a = f'/html/body/div[3]/div/div[1]/div[14]/a[{x+1}]'
        # driver.find_element(By.XPATH, a).click()
        next_button = driver.find_element(By.CSS_SELECTOR, f"a[id='{x+1}']")
        driver.execute_script("arguments[0].click();", next_button)
        time.sleep(2)

print('========= 작업 완료 =========')
# driver.close()


# n==50일 때, 타임 슬립이 없어 랜더링 전에 추출을 시도해서 에러 발생
print(scrap_cnt, page_cnt)
tit_xpath = '//*[@id="search_result"]/ul/li[*]/div[1]/div[1]/a'
no = 0

for x in range(1, page_cnt+1):
    print(f'========= {x} 페이지 작업 =========')
    mylist = driver.find_elements(By.XPATH, tit_xpath)
    
    for item in mylist:
        no += 1
        if no > scrap_cnt:
            break
        print(no, item.text)
    
    if no <= scrap_cnt:
        if no % 50 == 0:
            driver.find_element(By.CSS_SELECTOR, '.btn_next').click()
            continue
        
        next_button = driver.find_element(By.CSS_SELECTOR, f"a[id='{x+1}']")
        driver.execute_script("arguments[0].click();", next_button)
        #time.sleep(2)
    time.sleep(3)

print('========= 작업 완료 =========')
# driver.close()





from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import math, time

options = Options()
options.add_argument('--window-size=974,1047')
options.add_argument('--window-position=-7,0')
options.add_experimental_option("detach", True)

search = input('검색어를 입력하세요.')

URL = 'https://korean.visitkorea.or.kr/search/search_list.do?keyword=%EC%A0%9C%EC%A3%BC%EB%8F%84'+search

driver = webdriver.Chrome(options=options)
driver.get(URL)
time.sleep(3)


driver.find_element(By.CSS_SELECTOR, "#s_recommend > .more_view > a").click()


tit_xpath = '//*[@id="search_result"]/ul/li[*]/div[1]/div[1]/a'
contents_no = 0
page_no = 0

while True : 
    page_no += 1
    print(f'========= {page_no} 페이지 스크래핑 시작 =========')
    mylist = driver.find_elements(By.XPATH, tit_xpath)

    for i in range(len(mylist)):
        contents_no += 1
        print(contents_no, mylist[i].text)
    
    if no % 50 == 0:
        driver.find_element(By.CSS_SELECTOR, '.btn_next').click()
        time.sleep(3)
        continue

    try : # 다음 페이지 버튼이 없을 경우 예외처리
        next_button = driver.find_element(By.CSS_SELECTOR, f"a[id='{page_no+1}']")
        driver.execute_script("arguments[0].click();", next_button)
        time.sleep(3)
    except Exception as e:
        print('='*50)
        print(f'{page_no+1}페이지 버튼 클릭시 예외가 발생했습니다.', e)
        print('===== 데이터를 저장하고 프로그램을 종료합니다. =====')
        break     
        
print(f'========= 총 {page_no}페이지 {contents_no}개 데이터 스크래핑 완료 =========')





from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import math, time

options = Options()
options.add_argument('--window-size=974,1047')
options.add_argument('--window-position=-7,0')
options.add_experimental_option("detach", True)


URL = 'https://korean.visitkorea.or.kr/detail/rem_detail.do?cotid=f7e0eccb-bacc-4dc6-a54e-0aa1f585c69d&con_type=10100'

driver = webdriver.Chrome(options=options)
driver.get(URL)
time.sleep(3)


title = driver.find_element(By.ID, 'topTitle').text
title


contents = driver.find_elements(By.CLASS_NAME, 'txt_p')
print(contents[0].text)
print(contents)


contents = driver.find_elements(By.CLASS_NAME, 'txt_p')

contents_list = []
for item in contents:
    contents_list.append(item.text)
contents_list


#하나의 문자열로 통합
contents_merge = ' '.join(contents_list) 
contents_merge





from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options
import time, urllib.request


URL = 'https://korean.visitkorea.or.kr/detail/rem_detail.do?cotid=f7e0eccb-bacc-4dc6-a54e-0aa1f585c69d&con_type=10100='

driver = webdriver.Chrome(options=options)
driver.get(URL)
time.sleep(3)


pwd


import os
os.getcwd()


f_dir = input('이미지 저장 폴더명 : ')

now = time.localtime()
s = '%04d%02d%02d_%02d%02d%02d'%(now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec)
f_name = f_dir +'_' + s

os.makedirs(os.getcwd()+ '\\output\\'+f_name)


html_src = driver.page_source


html_dom = BeautifulSoup(html_src, 'lxml')
html_dom


mylist = html_dom.select('.txt_p img')
mylist


driver.execute_script("window.scrollTo(0, document.body.scrollHeight)")


html_src = driver.page_source


html_dom = BeautifulSoup(html_src, 'lxml')
html_dom


mylist = html_dom.select('.txt_p img')
len(mylist), mylist


img_list = [item['src'] for item in mylist]
img_list


urllib.request.urlretrieve(img_list[0],  'output/image.jpg')


# no = 0
for i, src in enumerate(img_list):
    # 다운로드  (주소, 파일이름)
    urllib.request.urlretrieve(src, f'{os.getcwd()}\\output\\{f_name}\\{i}.jpg')
    # no += 1





from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options
import urllib, math, time, os, pandas as pd

options = Options()
options.add_experimental_option("detach", True)

search = input('검색어:')
goal_cnt = int(input('스크래핑 할 건수는 몇건입니까?: '))
page_cnt = math.ceil(goal_cnt / 10)  # 크롤링 할 전체 페이지 수 

now = time.localtime()
date_format = '%04d%02d%02d'%(now.tm_year, now.tm_mon, now.tm_mday)
f_dir = f'{os.getcwd()}\\output\\{search}여행기사_{goal_cnt}건_{date_format}'
os.makedirs(f_dir, exist_ok=True) # 디렉토리가 미리 존재해도 에러나지 않도록

URL = 'https://korean.visitkorea.or.kr/search/search_list.do?keyword=%EC%A0%9C%EC%A3%BC%EB%8F%84'+search
driver = webdriver.Chrome(options=options)
driver.get(URL)
time.sleep(2)

# 여행기사 더보기 클릭
driver.find_element(By.CSS_SELECTOR, "#s_recommend > .more_view > a").click()
time.sleep(2)

contents_no = 1
title_list = []
contents_list = []
img_url_list = []

def page_work():
    global contents_no, goal_cnt, title_list, contents_list, img_url_list
    result = driver.find_elements(By.CSS_SELECTOR,'#search_result .tit>a')
    
    for i in range(0, len(result)):
        if contents_no <= goal_cnt :    
            
            # 페이지 변경으로인한 DOM객체 변경 문제로 소스 다시 추출하기
            result = driver.find_elements(By.CSS_SELECTOR,'#search_result .tit>a')
            title = result[i].text
            title_list.append(title)
            
            print(f'[{contents_no}] {title}') 
            
            result[i].send_keys(Keys.ENTER) # .click()은 에러 잘남    
            time.sleep(2)
            
            # 이미지 추출을 위해 미리 스크롤
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(2)
            
            # 상세페이지 소스 추출
            html = driver.page_source
            html_dom = BeautifulSoup(html, 'lxml')
          
            img_tag_list = html_dom.select('.img_typeBox img')
            img_url_list = [item['src'] for item in img_tag_list]

            contents = driver.find_elements(By.CLASS_NAME, 'txt_p')
            contents_merge = ' '.join([item.text for item in contents])        
            contents_list.append(contents_merge)           
            
            driver.back()
            time.sleep(2)     
            contents_no += 1
            
        if contents_no % 50 == 0: # 5page 단위 도달 시 넥스트 버튼 클릭
            driver.find_element(By.CSS_SELECTOR, '.btn_next').click()
            # continue

        
def file_export():
    global contents_no
    
    df = pd.DataFrame({"제목":title_list, "내용":contents_list}, index=None)

    filename = f'{search}여행기사_{goal_cnt}건_{date_format}.csv'
    fileaddr = f_dir+'\\'+filename
    
    if not os.path.exists(fileaddr):
        df.to_csv(fileaddr, index= False, mode='w', encoding='utf-8-sig')
    else:
        df.to_csv(fileaddr, index= False, mode='a', encoding='utf-8-sig', header=False)
    
    print(f'====== 콘텐츠 {contents_no-1}개, {filename} 파일 저장 완료 ======')
    
    img_no = 0
    for src in img_url_list:
        # 다운로드  (주소, 파일이름)
        img_no += 1
        urllib.request.urlretrieve(src, f'{f_dir}\\{page_no}_{img_no}.jpg')
        
    print(f'====== 이미지 {img_no}개 저장 완료 ======')

today = time.localtime()
print('스크래핑 프로그램 실행')

for page_no in range(1, page_cnt+1):    
    print(f'====== {page_no} 페이지 스크래핑 시작 ======')
    page_work()
    print(f'====== {page_no} 페이지 콘텐츠 저장 중 ======')
    file_export()
    print(f'====== {page_no} 페이지 스크래핑 완료 ======')
    if page_no < page_cnt:
        next_button = driver.find_element(By.CSS_SELECTOR, f"a[id='{page_no+1}']")
        driver.execute_script("arguments[0].click();", next_button)
        time.sleep(2)
                   
print('스크래핑 프로그램 종료')
driver.close()
